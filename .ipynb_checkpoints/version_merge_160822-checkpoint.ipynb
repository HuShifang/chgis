{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "# \n",
    "# /version_merge_160822.ipynb\n",
    "# /py_scripts/version_merge_160822.py\n",
    "#\n",
    "# A script for checking data integrity of CHGIS v5 and v6.  \n",
    "# The script takes a v5 .csv provided by Lex Berman and a v6 .csv generated using v6_consolidation_160814.ipynb (or the same-name .py script)\n",
    "# The .csv files are internally unique -- that is, there are no duplicate items within them -- and have several (but not all) fields in common\n",
    "# This script checks for duplicates between them, based on various fields including 'sys_id'\n",
    "# Columns containing Boolean values are appended to the end of rows, indicating whether fields are duplicates or not\n",
    "# There are several \"meta\" columns at the end, marking out rows that may be of interest\n",
    "# Note that while there are several rows where all relevant data save for 'sys_id' match, there do not appear to be any where only the 'sys_id' matches\n",
    "# The script has not been fully refactored, and can yet be optimized; however, it should still run relatively quickly.\n",
    "# Three .csv files are output -- one containing the entirety of both v5 and v6 together, one containing just v5 rows, and one containing just v6 rows\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# importing v5 and v6 tables as DataFrames\n",
    "v5 = pd.read_csv('input/v5_augment_2016-08-09.csv', low_memory=False)\n",
    "v6 = pd.read_csv('input/V6_input_draft_20160811.csv', low_memory=False)\n",
    "\n",
    "# creating a field indicating which version of CHGIS the given row came from\n",
    "v5['version'] = 5\n",
    "v6['version'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# merge v5 and v6 into a single DataFrame\n",
    "df = pd.concat([v5,v6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sf/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# check merged df for duplicates by sys_id, generate file with tagging\n",
    "mask_duplicates = df.duplicated(subset='sys_id', keep=False)\n",
    "mask_uniques = ~mask_duplicates\n",
    "duplicates = df[mask_duplicates]\n",
    "duplicates['sys_id_duplicate'] = True\n",
    "uniques = df[mask_uniques]\n",
    "uniques['sys_id_duplicate'] = False\n",
    "df_two = pd.concat([duplicates, uniques])\n",
    "df_two.to_csv('output/v5_and_v5_duplicates_and_uniques.csv')\n",
    "\n",
    "### following lines would output .csv's containing only the duplicates and uniques\n",
    "#duplicates.to_csv('output/v5_and_v6_primary_duplicates.csv')\n",
    "#uniques.to_csv('v5_and_v6_uniques.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for adding Boolean flags as fields to the DataFrame file\n",
    "def field_matcher(frame, v6_field, v5_field, new_field):\n",
    "    '''Simple function for comparing the specified fields from versions 5 and 6 of the CHGIS within the specified composite frame, outputting the Boolean result to a new field.  \n",
    "    Assumes that the CHGIS versions' tables which have been concatenated into the frame are already internally unique, i.e. that any matches found will be between versions, not within.  \n",
    "    Rather than iterate over rows, creates a Series of Boolean values which serves as a sort of mask for filtering out rows with duplicate and unique values in the specified fields. -- SF, 160817'''\n",
    "    mask_duplicates = frame.duplicated(subset=[v6_field, v5_field], keep=False)\n",
    "    mask_uniques = ~mask_duplicates\n",
    "    duplicates = frame[mask_duplicates]\n",
    "    duplicates[new_field] = True\n",
    "    uniques = frame[mask_uniques]\n",
    "    uniques[new_field] = False\n",
    "    return pd.concat([duplicates, uniques])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sf/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "/home/sf/anaconda3/lib/python3.5/site-packages/ipykernel/__main__.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    }
   ],
   "source": [
    "# performing duplicate checks field-by-field per 'Match_fields_from_V6_to_V5.csv'\n",
    "df_three = field_matcher(df_two, 'nm_py', 'nm_py', 'nm_py_duplicate')\n",
    "df_four = field_matcher(df_three, 'nm_simp', 'nm_simp', 'nm_simp_duplicate')\n",
    "df_five = field_matcher(df_four, 'nm_trad', 'nm_trad', 'nm_trad_duplicate')\n",
    "\n",
    "# trying to workaround failure to recognize matches\n",
    "df_five['x_coord'] = df_five['x_coord'].astype(str)\n",
    "df_five['y_coord'] = df_five['y_coord'].astype(str)\n",
    "df_six = field_matcher(df_five, 'x_coord', 'x_coord', 'x_coord_duplicate')\n",
    "df_seven = field_matcher(df_six, 'y_coord', 'y_coord', 'y_coord_duplicate')\n",
    "df_seven['x_coord'] = df_seven['x_coord'].astype(float)\n",
    "df_seven['y_coord'] = df_seven['y_coord'].astype(float)\n",
    "\n",
    "# resuming secondary checks\n",
    "df_eight = field_matcher(df_seven, 'pres_loc', 'pres_loc', 'pres_loc_duplicate')\n",
    "df_nine = field_matcher(df_eight, 'type_py', 'type_py', 'type_py_duplicate')\n",
    "df_ten = field_matcher(df_nine, 'type_simp', 'type_ch', 'type_simp/ch_duplicate')\n",
    "df_eleven = field_matcher(df_ten, 'beg_yr', 'beg', 'beg_yr_duplicate')\n",
    "df_twelve = field_matcher(df_eleven, 'end_yr', 'end', 'end_yr_duplicate')\n",
    "df_thirteen = field_matcher(df_twelve, 'obj_type', 'obj_type', 'obj_type_duplicate')\n",
    "df_thirteen['exact_match'] = df_thirteen.all(axis=1, bool_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### creating new columns for ease of filtering\n",
    "\n",
    "# NOTE that this can probably be refactored to use a slice of df_thirteen columns; \n",
    "# I was having problems slicing it right, however, so I used this less elegant approach, explicitly checking each field's value\n",
    "# The script runs sufficiently quickly as-is, however\n",
    "\n",
    "# Note that numpy.where() is used here, and NOT pandas.where() \n",
    "\n",
    "## if 'sys_id_duplicate' == False, but the other duplicate fields == True, then 'content_only' = True\n",
    "df_thirteen['content_only'] = np.where(\n",
    "    ((df_thirteen['sys_id_duplicate'] == False) &\n",
    "    ((df_thirteen['nm_py_duplicate'] == True) & \n",
    "    (df_thirteen['nm_simp_duplicate'] == True) & \n",
    "    (df_thirteen['nm_trad_duplicate'] == True) &\n",
    "    (df_thirteen['x_coord_duplicate'] == True) &\n",
    "    (df_thirteen['y_coord_duplicate'] == True) &\n",
    "    (df_thirteen['pres_loc_duplicate'] == True) &\n",
    "    (df_thirteen['type_py_duplicate'] == True) &\n",
    "    (df_thirteen['type_simp/ch_duplicate'] == True) &\n",
    "    (df_thirteen['beg_yr_duplicate'] == True) &\n",
    "    (df_thirteen['end_yr_duplicate'] == True) &\n",
    "    (df_thirteen['obj_type_duplicate'] == True))), 'True', 'False') \n",
    "\n",
    "## if 'sys_id'duplicate' == True, but the other duplicate fields == False, then 'id_only' = True\n",
    "df_thirteen['id_only'] = np.where(\n",
    "    ((df_thirteen['sys_id_duplicate'] == True) & \n",
    "    ((df_thirteen['nm_py_duplicate'] == False) & \n",
    "    (df_thirteen['nm_simp_duplicate'] == False) & \n",
    "    (df_thirteen['nm_trad_duplicate'] == False) &\n",
    "    (df_thirteen['x_coord_duplicate'] == False) &\n",
    "    (df_thirteen['y_coord_duplicate'] == False) &\n",
    "    (df_thirteen['pres_loc_duplicate'] == False) &\n",
    "    (df_thirteen['type_py_duplicate'] == False) &\n",
    "    (df_thirteen['type_simp/ch_duplicate'] == False) &\n",
    "    (df_thirteen['beg_yr_duplicate'] == False) &\n",
    "    (df_thirteen['end_yr_duplicate'] == False) &\n",
    "    (df_thirteen['obj_type_duplicate'] == False))), 'True', 'False') \n",
    "\n",
    "df_final = df_thirteen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# outputting .csv with v5 AND v6\n",
    "df_final.to_csv('output/V5andV6_output.csv')\n",
    "\n",
    "# masking, outputting v5 alone\n",
    "v5_mask = df_final['version'] == 5\n",
    "v5_final = df_final[v5_mask]\n",
    "v5_final.to_csv('output/V5_output.csv')\n",
    "\n",
    "# masking, outputting v6 alone\n",
    "v6_mask = df_final['version'] == 6\n",
    "v6_final = df_final[v6_mask]\n",
    "v6_final.to_csv('output/V6_output.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
