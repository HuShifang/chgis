{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "\n",
    "# Script for comparing an incoming .csv file with a target .csv file (e.g. the CHGIS).\n",
    "# Matches primarily on name, but optionally on other fields too\n",
    "# Only non-core library used is pandas, which can be installed via pip or as part of a Python distribution (e.g. Anaconda)\n",
    "# by Stephen Ford (stephen.p.ford@gmail.com)\n",
    "\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from collections import OrderedDict\n",
    "from pprint import pprint\n",
    "import os.path\n",
    "\n",
    "# suppressing SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Initializing the lists and dictionaries that will be used:\n",
    "# 1. to map the fields in the incoming and target .csv files to the standard output fields\n",
    "# 2. to drop the standard output fields that the user chooses not to use\n",
    "# 3. to put the final output fields into their desired, standard order\n",
    "\n",
    "# initializing list of incoming_fields \n",
    "incoming_fields = []\n",
    "\n",
    "# defining dictionary that stores the standard incoming field names along with a natural English description, for use in user prompts\n",
    "incoming_description = OrderedDict([\n",
    "    ('input_id', 'a unique ID'),\n",
    "    ('input_nm_py',\"a name in pinyin\"),\n",
    "    ('input_nm_simp',\"a name in simplified Chinese characters (简体字)\"),\n",
    "    ('input_nm_trad',\"a name in traditional Chinese characters (繁體字)\"),\n",
    "    ('input_type_py',\"an administrative type in pinyin (e.g. 'Xian')\"),\n",
    "    ('input_type_ch',\"an administrative type in Chinese (simplified) characters (e.g. '县')\"),\n",
    "    ('input_year_beg',\"a beginning year\"),\n",
    "    ('input_year_end',\"an ending year\"),\n",
    "    ('input_dynasty',\"a dynasty\"),\n",
    "    ('input_other_id',\"another, alternate unique ID\"),\n",
    "    ('input_prnt',\"the parent administrative unit's name\"),\n",
    "    ('input_obj_type',\"a geospatial type (Point/Vector/Polygon)\"),\n",
    "    ('input_x_coord',\"an x coordinate\"),\n",
    "    ('input_y_coord',\"a y coordinate\")\n",
    "])\n",
    "    \n",
    "\n",
    "# initializing dictionary that will store as key-value pairs the original .csv's fields and what they're renamed in the final output, respectively \n",
    "incoming_mapping = OrderedDict([])\n",
    "\n",
    "# declaring the list of output .csv field names that will derive from the new data\n",
    "# i.e., all those fields whose names will be 'input_*' in the final .csv\n",
    "final_incoming_fields = list(incoming_description.keys())\n",
    "\n",
    "# initializing list of actual target fields\n",
    "target_fields = []\n",
    "\n",
    "# initializing list of default CHGIS fields (taken from v5_augment_2016-08-09.csv)\n",
    "default_target_fields_v5 = [\n",
    "    'seq', \n",
    "    'sys_id', \n",
    "    'src', \n",
    "    'nm_py', \n",
    "    'nm_simp', \n",
    "    'nm_trad', \n",
    "    'x_coord', \n",
    "    'y_coord', \n",
    "    'pres_loc', \n",
    "    'type_py', \n",
    "    'type_ch', \n",
    "    'beg', \n",
    "    'end', \n",
    "    'obj_type',\n",
    "    'prnt_id', \n",
    "    'prnt_sysid', \n",
    "    'prnt_simp', \n",
    "    'prnt_py'\n",
    "]\n",
    "\n",
    "default_target_fields_v6 = [\n",
    " 'beg_rule',\n",
    " 'beg_type',\n",
    " 'beg_yr',\n",
    " 'checker',\n",
    " 'compiler',\n",
    " 'end_rule',\n",
    " 'end_type',\n",
    " 'end_yr',\n",
    " 'entry_date',\n",
    " 'filename',\n",
    " 'geo_comp',\n",
    " 'geo_src',\n",
    " 'level',\n",
    " 'mdb_id',\n",
    " 'nm_py',\n",
    " 'nm_simp',\n",
    " 'nm_trad',\n",
    " 'note_id',\n",
    " 'obj_type',\n",
    " 'orig_id',\n",
    " 'pres_loc',\n",
    " 'sys_id',\n",
    " 'type_py',\n",
    " 'type_simp',\n",
    " 'x_coord',\n",
    " 'y_coord'\n",
    "]\n",
    "\n",
    "\n",
    "target_description = OrderedDict([\n",
    "    ('tgaz_sys_id','a unique ID'),\n",
    "    ('tgaz_nm_py', \"a name in pinyin\"),\n",
    "    ('tgaz_nm_simp',\"a name in simplified Chinese characters (简体字)\"),\n",
    "    ('tgaz_nm_trad',\"a name in traditional Chinese characters (繁體字)\"),\n",
    "    ('tgaz_beg',\"a beginning year\"),\n",
    "    ('tgaz_end',\"an ending year\"),\n",
    "    ('tgaz_data_source', \"the source of the data\"),\n",
    "    ('tgaz_obj_type',\"a geospatial type (Point/Vector/Polygon)\"),\n",
    "    ('tgaz_pres_loc',\"the place's present-day name\"),\n",
    "    ('tgaz_prnt_id',\"the parent administrative unit's unique ID (NOT prefixed 'hvd_')\"),\n",
    "    ('tgaz_prnt_py',\"the parent administrative unit's name in pinyin\"),\n",
    "    ('tgaz_prnt_simp',\"the parent administrative unit's name in simplified Chinese characters (简体字)\"),\n",
    "    ('tgaz_prnt_sysid',\"the parent administrative unit's unique ID (prefxed 'hvd_')\"),\n",
    "    ('tgaz_type_ch',\"an administrative type in Chinese (simplified) characters (e.g. 县)\"),\n",
    "    ('tgaz_type_py',\"an administrative type in pinyin (e.g. 'Xian')\"),\n",
    "    ('tgaz_x_coord',\"an x coordinate\"),\n",
    "    ('tgaz_y_coord',\"a y coordinate\")\n",
    "])\n",
    "\n",
    "# initializing dictionary that will store as key-value pairs the original target .csv's fields and what they're renamed in the final output\n",
    "target_mapping = OrderedDict([])\n",
    "\n",
    "# initializing list of tgaz fields (i.e. the standardized output-form of the CHGIS fields)\n",
    "final_target_fields = list(target_description.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for selecting .csv files for manipulation\n",
    "\n",
    "def csv_picker():\n",
    "    ''' Function for checking whether user input path 1) is that of a valid file, and 2) is of a file ending with '.csv'\n",
    "        Prompts for re-entry if entry is invalid.\n",
    "        Returns a pandas DataFrame constructed from the valid .csv file\n",
    "    '''\n",
    "    path_name = input()\n",
    "\n",
    "    # checking that the path is a valid filename, and prompting for re-entry if not\n",
    "    while not (os.path.isfile(path_name)):\n",
    "        print(\"Not a valid filename.  Please try again:\")\n",
    "        path_name = input()\n",
    "        \n",
    "    # checking that the valid filename ends in .csv, prompting for re-entry if not\n",
    "    while not path_name.endswith('.csv'):\n",
    "        print(\"Filename does not end in .csv -- please try again:\")\n",
    "        path_name = input()\n",
    "\n",
    "    print(\"\\nThank you -- path %s is valid.\\n\" % path_name)\n",
    "    \n",
    "    # storing only the file's basename, for use in user prompts\n",
    "    name = os.path.basename(path_name)\n",
    "    return pd.read_csv(path_name, low_memory=False), name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for mapping the input .csv's fields to the desired, standardized output fields\n",
    "def field_mapper(std_field, std_field_description, frame, fields, name, mapping):\n",
    "    ''' Function that will prompt user to manually map fields of the input .csv to standardized output fields.\n",
    "        Name changes will be made in-place (i.e. in the DataFrame -- the .csv will be untouched).\n",
    "        If user fails to enter anything for the given mapping, that field will dropped from the final output file.        \n",
    "    '''\n",
    "    \n",
    "    print(\"\\nPlease enter the field of the incoming file %s that contains %s (this will be renamed '%s' in the output .csv):\" % (name, std_field_description, std_field))\n",
    "    orig_field = input()\n",
    "    \n",
    "    # prompts for re-entering the input field if 1) it is not one of the column names and 2) it isn't an empty string\n",
    "    while (not orig_field in list(frame.columns)) and (orig_field):\n",
    "        print(\"\\nNot a valid column name.  Please try again:\")\n",
    "        orig_field = input()\n",
    "        \n",
    "    # simply exit if the user pressed enter, bypassing the mapping, or perform the mapping if a valid field name has been entered\n",
    "    if orig_field:\n",
    "        mapping.update({orig_field:std_field})\n",
    "        frame.rename(columns={orig_field:std_field}, inplace=True)\n",
    "        fields += [std_field]\n",
    "    return fields, mapping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def name_checker(name_fields, fields, prefix, frame):\n",
    "    ''' Function will confirm that at least one name field has been entered, and prompt user to remap the three\n",
    "        name fields until at least one is a valid entry. Updates the DataFrame in-place and returns the updated field list.\n",
    "    '''\n",
    "    \n",
    "    while (((name_fields[0] in fields) or (name_fields[1] in fields) or (name_fields[2] in fields)) == False):\n",
    "        print(\"At least one name field needs to be specified. Please try again.\")\n",
    "        # counter ensures that name fields are inserted at correct place in sequence\n",
    "        counter = 1\n",
    "        for name_field in name_fields:\n",
    "            print(\"Please enter the field that will be labeled '%s' in the output .csv:\" % name_field)\n",
    "            orig_field = input()\n",
    "            if orig_field:\n",
    "                if (orig_field in list(frame.columns)):\n",
    "                    frame.rename(columns={orig_field:name_field}, inplace=True)\n",
    "                    fields.insert(counter, name_field)\n",
    "                    counter+=1\n",
    "                else: \n",
    "                    print(\"Input not accepted -- field not found in data.\")\n",
    "    return fields\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# offering user choice of strict or fuzzy name-matching\n",
    "\n",
    "def merge_chooser(target_field, incoming_field):\n",
    "    ''' Function called only if the user selects to merge on traditional characters 繁體字 or simplified characters 简体字\n",
    "        Lets user choose whether to do a strict or fuzzy merge.\n",
    "        In a fuzzy merge, only the first two characters of the Chinese names will be checked against one another.\n",
    "    '''\n",
    "    print(\n",
    "    '''\n",
    "Please indicate, by entering a numerical digit 1-2, whether you wish to do a strict or fuzzy match of names:\n",
    "    1. Strict matching (e.g. '張掖' matches '張掖', but '張掖' does not match '張掖居延屬國')\n",
    "    2. Fuzzy matching (e.g. '張掖' matches '張掖', and '張掖' also matches '張掖居延屬國')\n",
    "    ''')\n",
    "\n",
    "    accepted = False\n",
    "    choice = input()\n",
    "\n",
    "    while accepted == False:\n",
    "        if choice == '1':\n",
    "            print('Proceeding with strict matching of names.')\n",
    "            accepted = True\n",
    "            mode = 'strict'\n",
    "            df = target.merge(incoming, how='outer', left_on=target_field, right_on=incoming_field, indicator=True)\n",
    "            return df, mode\n",
    "        elif choice == '2':\n",
    "            print('Proceeding with fuzzy matching of names.')\n",
    "            accepted = True\n",
    "            mode = 'fuzzy'\n",
    "            target['fuzzy_nm'] = target[target_field].map(lambda x: str(x)[:2])\n",
    "            incoming['fuzzy_nm'] = incoming[incoming_field].map(lambda x: str(x)[:2])\n",
    "            df = target.merge(incoming, how='outer', on='fuzzy_nm', indicator=True)\n",
    "            return df, mode\n",
    "        else:\n",
    "            print(\"\\nNot a valid response.  Please try again:\\n\")\n",
    "            choice = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def coordinate_matcher(coords, frame, fields):\n",
    "    ''' Function that performs a strict or fuzzy matching of spatial coordinates.  Returns a string indicating the type of match performed.\n",
    "        The matches' results are added to the DataFrame within the function.\n",
    "    '''\n",
    "    \n",
    "    print(\n",
    "        '''\n",
    "    Please indicate, by entering a numerical digit 1-2, whether you wish to do a strict or fuzzy match of spatial coordinates:\n",
    "        1. Strict matching (requires exact match, e.g. '119.64656' does NOT match '119.646560', '119.64657', or '119.325')\n",
    "        2. Fuzzy matching (requires only that rounded numbers match -- you specify the number of decimal places)\n",
    "        ''')\n",
    "\n",
    "    accepted = False\n",
    "    choice = input()\n",
    "\n",
    "    while accepted == False:\n",
    "        if choice == '1':\n",
    "            print('Proceeding with strict matching of spatial coordinates.')\n",
    "            accepted = True\n",
    "            for coord in coords:\n",
    "                frame['out_%s_coord_match' % coord] = frame['input_%s_coord' % coord] == frame['tgaz_%s_coord' % coord]\n",
    "                fields += ['out_%s_coord_match' % coord]\n",
    "            decimal_place = None\n",
    "            return \"strict\", decimal_place\n",
    "        elif choice == '2':\n",
    "            print(\"Proceeding with fuzzy matching of spatial coordinates.\")\n",
    "            accepted = True\n",
    "            print('''Please enter a number indicating the number of decimal places to which to round the decimal coordinate value.\n",
    "                      For example, if you enter 0, 117.91 and 118.08 will round to 118 and match; if you enter 1, 117.91 will round to 117.9 and 118.08 will round to 118.1, and they won't match.\n",
    "                      Be advised that coordinates rarely have more than 7 decimal places of precision.\n",
    "                  ''')\n",
    "            decimal_place = input()\n",
    "            try: \n",
    "                for coord in coords:\n",
    "                    frame['fuzzy_out_%s_coord_match' % coord] = frame['input_%s_coord' % coord].map(lambda x: round(x, int(decimal_place))) == frame['tgaz_%s_coord' % coord].map(lambda x: round(x, int(decimal_place)))\n",
    "                    fields += ['fuzzy_out_%s_coord_match' % coord]\n",
    "                return \"fuzzy\", decimal_place\n",
    "                                                                                                \n",
    "            except:\n",
    "                print(\"Not a valid response. Defaulting to 0 (integer-rounding).\")\n",
    "                for coord in coords:\n",
    "                    frame['fuzzy_out_%s_coord_match' % coord] = frame['input_%s_coord' % coord].map(lambda x: round(x, 0)) == frame['tgaz_%s_coord' % coord].map(lambda x: round(x, 0))\n",
    "                    fields += ['fuzzy_out_%s_coord_match' % coord]\n",
    "                return \"fuzzy\", decimal_place\n",
    "        else:\n",
    "            print(\"\\nNot a valid response.  Please try again:\\n\")\n",
    "            choice = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def title_caser(fields, actual_fields, frame):\n",
    "    '''Very simple function that title-cases the contents of the given fields, provided that they are actually used in the DataFrame \n",
    "    '''   \n",
    "    for field in fields:\n",
    "        if field in actual_fields:\n",
    "            frame[field] = frame[field].map(lambda x: str(x).title())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type the path of the incoming .csv file (with extension):\n",
      "/home/sf/chgis/input/sample_data/lexdata.txt.data.csv\n",
      "\n",
      "Thank you -- path /home/sf/chgis/input/sample_data/lexdata.txt.data.csv is valid.\n",
      "\n",
      "Please type the path of the target .csv file (with extension):\n",
      "/home/sf/chgis/input/V6_input_draft_20160811.csv\n",
      "\n",
      "Thank you -- path /home/sf/chgis/input/V6_input_draft_20160811.csv is valid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# soliciting files for comparison; presumption is that second file entered will be the CHGIS v5 in .csv format\n",
    "print(\"Please type the path of the incoming .csv file (with extension):\")\n",
    "incoming, incoming_name = csv_picker()\n",
    "\n",
    "print(\"Please type the path of the target .csv file (with extension):\")\n",
    "target, target_name = csv_picker()\n",
    "\n",
    "\n",
    "### DEV ONLY\n",
    "# DongHan\n",
    "#incoming = pd.read_csv(\"/home/sf/chgis/input/sample_data/Donghan_2014-10-02_copy.csv\", low_memory=False)\n",
    "#incoming_name = \"Donghan_2014-10-02_copy.csv\"\n",
    "\n",
    "# /home/sf/chgis/input/sample_data/Donghan_2014-10-02_copy.csv\n",
    "# /home/sf/chgis/input/sample_data/lexdata.txt.data.csv\n",
    "\n",
    "\n",
    "# v5\n",
    "#target = pd.read_csv(\"/home/sf/chgis/input/v5_augment_2016-08-09.csv\", low_memory=False)\n",
    "#target_name = \"v5_augment_2016-08-09.csv\"\n",
    "\n",
    "\n",
    "# v6\n",
    "#target = pd.read_csv(\"/home/sf/chgis/input/V6_input_draft_20160811.csv\", low_memory=False)\n",
    "#target_name = \"V6_input_draft_20160811.csv\"\n",
    "# /home/sf/chgis/input/v5_augment_2016-08-09.csv\n",
    "# /home/sf/chgis/input/V6_input_draft_20160811.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please indicate by entering '1' or '2' whether you wish to use a default mapping of CHGIS fields, or wish to manually map fields.\n",
      "    \n",
      "1. Use the default mapping -- presumes the target file has either of the following sets of columns:\n",
      "\n",
      "  CHGIS v5 standard fields:\n",
      "  ['seq', 'sys_id', 'src', 'nm_py', 'nm_simp', 'nm_trad', 'x_coord', 'y_coord', 'pres_loc', 'type_py', 'type_ch', 'beg', 'end', 'obj_type', 'prnt_id', 'prnt_sysid', 'prnt_simp', 'prnt_py']\n",
      "  \n",
      "  CHGIS v6 (Aug. 2016 draft) standard fields:\n",
      "  ['beg_rule', 'beg_type', 'beg_yr', 'checker', 'compiler', 'end_rule', 'end_type', 'end_yr', 'entry_date', 'filename', 'geo_comp', 'geo_src', 'level', 'mdb_id', 'nm_py', 'nm_simp', 'nm_trad', 'note_id', 'obj_type', 'orig_id', 'pres_loc', 'sys_id', 'type_py', 'type_simp', 'x_coord', 'y_coord']\n",
      "    \n",
      "\n",
      "2. Use a manual mapping -- you will be prompted to indicate which column from the file should map to which output column.\n",
      "\n",
      "2\n",
      "Now, please specify fields from the CHGIS (match-receiving) data that will be included in the final spreadsheet.\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a unique ID (this will be renamed 'tgaz_sys_id' in the output .csv):\n",
      "sys_id\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a name in pinyin (this will be renamed 'tgaz_nm_py' in the output .csv):\n",
      "nm_py\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a name in simplified Chinese characters (简体字) (this will be renamed 'tgaz_nm_simp' in the output .csv):\n",
      "nm_simp\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a name in traditional Chinese characters (繁體字) (this will be renamed 'tgaz_nm_trad' in the output .csv):\n",
      "nm_trad\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a beginning year (this will be renamed 'tgaz_beg' in the output .csv):\n",
      "beg_yr\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains an ending year (this will be renamed 'tgaz_end' in the output .csv):\n",
      "end_yr\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the source of the data (this will be renamed 'tgaz_data_source' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a geospatial type (Point/Vector/Polygon) (this will be renamed 'tgaz_obj_type' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the place's present-day name (this will be renamed 'tgaz_pres_loc' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the parent administrative unit's unique ID (NOT prefixed 'hvd_') (this will be renamed 'tgaz_prnt_id' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the parent administrative unit's name in pinyin (this will be renamed 'tgaz_prnt_py' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the parent administrative unit's name in simplified Chinese characters (简体字) (this will be renamed 'tgaz_prnt_simp' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains the parent administrative unit's unique ID (prefxed 'hvd_') (this will be renamed 'tgaz_prnt_sysid' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains an administrative type in Chinese (simplified) characters (e.g. 县) (this will be renamed 'tgaz_type_ch' in the output .csv):\n",
      "type_simp\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains an administrative type in pinyin (e.g. 'Xian') (this will be renamed 'tgaz_type_py' in the output .csv):\n",
      "type_py\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains an x coordinate (this will be renamed 'tgaz_x_coord' in the output .csv):\n",
      "x_coord\n",
      "\n",
      "Please enter the field of the incoming file V6_input_draft_20160811.csv that contains a y coordinate (this will be renamed 'tgaz_y_coord' in the output .csv):\n",
      "y_coord\n"
     ]
    }
   ],
   "source": [
    "### presenting user with choice of a default mapping of CHGIS fields (based on v5_augment_2016-08-09.csv) or of manually entering their own mapping\n",
    "\n",
    "print('''Please indicate by entering '1' or '2' whether you wish to use a default mapping of CHGIS fields, or wish to manually map fields.\n",
    "    \n",
    "1. Use the default mapping -- presumes the target file has either of the following sets of columns:\n",
    "\n",
    "  CHGIS v5 standard fields:\n",
    "  %s\n",
    "  \n",
    "  CHGIS v6 (Aug. 2016 draft) standard fields:\n",
    "  %s\n",
    "    \n",
    "\n",
    "2. Use a manual mapping -- you will be prompted to indicate which column from the file should map to which output column.\n",
    "''' % (str(default_target_fields_v5), str(default_target_fields_v6)))\n",
    "\n",
    "accepted = False\n",
    "mapping = input()\n",
    "\n",
    "while accepted == False:\n",
    "    # checks them against one another using comparison of sets (which are collections of unordered, unique items)\n",
    "    if (mapping == '1'):\n",
    "        # if the default target fields from CHGIS v5 are a subset of the target's fields, rename and use them while dropping the fields not in CHGIS v5 \n",
    "        if all(item in set(list(target.columns)) for item in set(default_target_fields_v5)):\n",
    "            # manually renaming one exception to the following pattern\n",
    "            target.rename(columns={'src':'data_source'}, inplace=True)\n",
    "\n",
    "            # manually dropping the 'seq' field\n",
    "            del target['seq']\n",
    "            \n",
    "            # generating the target_mapping using a dictionary comprehension\n",
    "            target_mapping = OrderedDict([(key, 'tgaz_%s' % key) for key in target.columns]) #{key:'tgaz_%s' % key for key in target.columns}\n",
    "            \n",
    "            # renaming the target fields in-place to conform to output specifications\n",
    "            #target.columns = list(target_mapping.values())\n",
    "            target.rename(columns={key:value for key,value in target_mapping.items()}, inplace=True)\n",
    "            \n",
    "            #target.columns = ['tgaz_%s' % x for x in target.columns]\n",
    "            \n",
    "            # dropping excess fields\n",
    "            target = target[final_target_fields]\n",
    "            target_fields = list(target.columns)\n",
    "            accepted = True\n",
    "      \n",
    "        # if the default target fields from CHGIS v6 are a subset of the target's fields, rename and use them while dropping the fields not in CHGIS v6\n",
    "        elif all(item in set(list(target.columns)) for item in set(default_target_fields_v6)):\n",
    "            # renaming in preparation for \"tgaz_\" prefixing\n",
    "            target.rename(columns={'geo_src':'data_source', 'type_simp':'type_ch', 'beg_yr':'beg', 'end_yr':'end' }, inplace=True)\n",
    "            \n",
    "            # dropping the difference of the two sets -- i.e. the fields that will not be used\n",
    "            # for item in (set(list(target.columns))\n",
    "            target_mapping = OrderedDict([(key, 'tgaz_%s' % key) for key in target.columns])\n",
    "            #target_mapping = {key:'tgaz_%s' % key for key in target.columns}\n",
    "            target.rename(columns={key:value for key,value in target_mapping.items()}, inplace=True)\n",
    "            #target.columns = list(target_mapping.values())\n",
    "            \n",
    "            # prefixing\n",
    "            #target.columns = ['tgaz_%s' % x for x in target.columns]\n",
    "            \n",
    "            # eliminating parent fields (since v6 data does not include parent information)\n",
    "            for parent_item in ['tgaz_prnt_id', 'tgaz_prnt_sysid', 'tgaz_prnt_simp', 'tgaz_prnt_py']:\n",
    "                final_target_fields.remove(parent_item) \n",
    "    \n",
    "            # dropping other excess fields\n",
    "            target = target[final_target_fields]\n",
    "            target_fields = list(target.columns)\n",
    "            accepted = True\n",
    "        \n",
    "        else: \n",
    "            # EDIT THE FOLLOWING TEXT\n",
    "            print(\"The columns in the selected CHGIS spreadsheet do not precisely match expectations. \\n\\n Proceeding with manual mapping.\")\n",
    "            for field, description in target_description.items():\n",
    "                target_fields, target_mapping = field_mapper(field, description, target, target_fields, target_name, target_mapping)  \n",
    "            target_fields = name_checker(['tgaz_nm_py', 'tgaz_nm_simp', 'tgaz_nm_trad'], target_fields, \"tgaz\", target)\n",
    "            target = target[target_fields]\n",
    "            accepted = True\n",
    "    elif (mapping == '2'):\n",
    "        print(\"Now, please specify fields from the CHGIS (match-receiving) data that will be included in the final spreadsheet.\\n\")\n",
    "        for field, description in target_description.items():\n",
    "            target_fields, target_mapping = field_mapper(field, description, target, target_fields, target_name, target_mapping)  \n",
    "        target_fields = name_checker(['tgaz_nm_py', 'tgaz_nm_simp', 'tgaz_nm_trad'], target_fields, \"tgaz\", target)\n",
    "        target = target[target_fields]\n",
    "        accepted = True\n",
    "    else:\n",
    "        print(\"\\nNot a valid response.  Please try again:\\n\")\n",
    "        mapping = input()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print(target.columns)\n",
    "#print(target)\n",
    "#print(target['tgaz_nm_py'].dtype)\n",
    "print(target_fields)\n",
    "print(target)\n",
    "print(target_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now we will specify fields from the incoming (match-making) data that will be included in the final spreadsheet.\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a unique ID (this will be renamed 'input_id' in the output .csv):\n",
      "id\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a name in pinyin (this will be renamed 'input_nm_py' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a name in simplified Chinese characters (简体字) (this will be renamed 'input_nm_simp' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a name in traditional Chinese characters (繁體字) (this will be renamed 'input_nm_trad' in the output .csv):\n",
      "name\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains an administrative type in pinyin (e.g. 'Xian') (this will be renamed 'input_type_py' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains an administrative type in Chinese (simplified) characters (e.g. '县') (this will be renamed 'input_type_ch' in the output .csv):\n",
      "level\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a beginning year (this will be renamed 'input_year_beg' in the output .csv):\n",
      "start\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains an ending year (this will be renamed 'input_year_end' in the output .csv):\n",
      "end\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a dynasty (this will be renamed 'input_dynasty' in the output .csv):\n",
      "dynasty\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains another, alternate unique ID (this will be renamed 'input_other_id' in the output .csv):\n",
      "oldid\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains the parent administrative unit's name (this will be renamed 'input_prnt' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a geospatial type (Point/Vector/Polygon) (this will be renamed 'input_obj_type' in the output .csv):\n",
      "\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains an x coordinate (this will be renamed 'input_x_coord' in the output .csv):\n",
      "JingDu\n",
      "\n",
      "Please enter the field of the incoming file lexdata.txt.data.csv that contains a y coordinate (this will be renamed 'input_y_coord' in the output .csv):\n",
      "WeuDu\n"
     ]
    }
   ],
   "source": [
    "# renaming fields in the incoming DataFrame to conform to specifications\n",
    "\n",
    "print(\"Now we will specify fields from the incoming (match-making) data that will be included in the final spreadsheet.\\n\")\n",
    "for field, description in incoming_description.items():\n",
    "    incoming_fields, incoming_mapping = field_mapper(field, description, incoming, incoming_fields, incoming_name, incoming_mapping)    \n",
    "\n",
    "incoming_fields = name_checker(['input_nm_py', 'input_nm_simp', 'input_nm_trad'], incoming_fields, \"input\", incoming)\n",
    "incoming = incoming[incoming_fields]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### titlecasing string values in pinyin fields to avoid spurious mismatches; could be refactored later\n",
    "\n",
    "#for field in ['tgaz_nm_py', 'tgaz_type_py']:\n",
    "#    if field in target_fields:\n",
    "#        target[field] = target[field].map(lambda x: str(x).title())\n",
    "        \n",
    "#for field in ['input_nm_py', 'input_type_py']:\n",
    "#    if field in incoming_fields:\n",
    "#        incoming[field] = incoming[]\n",
    "#target['tgaz_nm_py'] = target['tgaz_nm_py'].map(lambda x: str(x).title())\n",
    "#target['tgaz_type_py'] = target['tgaz_type_py'].map(lambda x: str(x).title())\n",
    "#incoming['input_nm_py'] = incoming['input_nm_py'].map(lambda x: str(x).title())\n",
    "#incoming['input_type_py'] = incoming['input_nm_py'].map(lambda x: str(x).title())\n",
    "\n",
    "title_caser(['tgaz_nm_py', 'tgaz_type_py'], target_fields, target)\n",
    "title_caser(['input_nm_py', 'input_type_py'], incoming_fields, incoming)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Thank you. Now, please indicate, by entering a numerical digit 1-3, which of the following names you wish to make the primary key for comparing data:\n",
      "    1. Name in complex/traditional Chinese characters 繁体字\n",
      "    2. Name in simplified Chinese characters 简体字\n",
      "    3. Name in pinyin 拼音\n",
      "    \n",
      "1\n",
      "\n",
      "Using name in complex/traditional Chinese characters 繁体字 as primary matching key.\n",
      "\n",
      "Please indicate, by entering a numerical digit 1-2, whether you wish to do a strict or fuzzy match of names:\n",
      "    1. Strict matching (e.g. '張掖' matches '張掖', but '張掖' does not match '張掖居延屬國')\n",
      "    2. Fuzzy matching (e.g. '張掖' matches '張掖', and '張掖' also matches '張掖居延屬國')\n",
      "    \n",
      "2\n",
      "Proceeding with fuzzy matching of names.\n"
     ]
    }
   ],
   "source": [
    "# soliciting user choice regarding which name field to take as primary\n",
    "print(\n",
    "    '''\n",
    "Thank you. Now, please indicate, by entering a numerical digit 1-3, which of the following names you wish to make the primary key for comparing data:\n",
    "    1. Name in complex/traditional Chinese characters 繁体字\n",
    "    2. Name in simplified Chinese characters 简体字\n",
    "    3. Name in pinyin 拼音\n",
    "    '''\n",
    ")\n",
    "\n",
    "accepted = False\n",
    "choice = input()\n",
    "\n",
    "while accepted == False:\n",
    "    if ((choice == '1') and ('input_nm_trad' in incoming_fields)):\n",
    "        print(\"\\nUsing name in complex/traditional Chinese characters 繁体字 as primary matching key.\")\n",
    "        accepted = True\n",
    "        match_key = 'nm_trad'\n",
    "        incoming_name_match_field = 'input_nm_trad'\n",
    "        target_name_match_field = 'tgaz_nm_trad'\n",
    "        df, name_mode = merge_chooser(target_name_match_field, incoming_name_match_field)\n",
    "    elif ((choice == '2') and ('input_nm_simp' in incoming_fields)):\n",
    "        print(\"\\nUsing name in simplified Chinese characters 简体字 as primary matching key.\")\n",
    "        accepted = True\n",
    "        match_key = 'nm_simp'\n",
    "        incoming_name_match_field = 'input_nm_simp'\n",
    "        target_name_match_field = 'tgaz_nm_simp'\n",
    "        df, name_mode = merge_chooser(target_name_match_field, incoming_name_match_field)\n",
    "    elif ((choice == '3') and ('input_nm_py' in incoming_fields)):\n",
    "        print(\"\\nUsing name in pinyin 拼音 as primary matching key.\")\n",
    "        accepted = True\n",
    "        match_key = 'nm_py'\n",
    "        incoming_name_match_field = 'input_nm_py'\n",
    "        target_name_match_field = 'tgaz_nm_py'\n",
    "        name_mode = 'strict'\n",
    "        print('Fuzzy matching is not currently supported for pinyin names.  Proceeding with strict matching.')\n",
    "        df = target.merge(incoming, how='outer', left_on=target_name_match_field, right_on=incoming_name_match_field, indicator=True)\n",
    "        \n",
    "    else:\n",
    "        print(\"\\nNot a valid response.  Please try again, entering a choice corresponding to a valid field:\\n\")\n",
    "        choice = input()\n",
    "        \n",
    "output_fields = []            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# removing rows that are only present in the CHGIS file\n",
    "df = df[df['_merge'] != 'left_only']\n",
    "\n",
    "# renaming merge indicators for legibility\n",
    "df = df.replace(to_replace='both', value='found')\n",
    "df = df.replace(to_replace='right_only', value='not_found')\n",
    "df.rename(columns={'_merge':'match'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Please indicate, by entering a numerical digit 1-2, whether you wish to do a strict or fuzzy match of spatial coordinates:\n",
      "        1. Strict matching (requires exact match, e.g. '119.64656' does NOT match '119.646560', '119.64657', or '119.325')\n",
      "        2. Fuzzy matching (requires only that rounded numbers match -- you specify the number of decimal places)\n",
      "        \n",
      "2\n",
      "Proceeding with fuzzy matching of spatial coordinates.\n",
      "Please enter a number indicating the number of decimal places to which to round the decimal coordinate value.\n",
      "                      For example, if you enter 0, 117.91 and 118.08 will round to 118 and match; if you enter 1, 117.91 will round to 117.9 and 118.08 will round to 118.1, and they won't match.\n",
      "                      Be advised that coordinates rarely have more than 7 decimal places of precision.\n",
      "                  \n",
      "2\n"
     ]
    }
   ],
   "source": [
    "#### Abandoning comprehensive for-loop for sake of year matching; manually stringifying coords instead\n",
    "\n",
    "# converting all fields to strings for ease of comparison\n",
    "#for field in list(df.columns):\n",
    "#    df[field] = df[field].astype(str)\n",
    "spatial_coords = ['input_x_coord', 'input_y_coord', 'tgaz_x_coord', 'tgaz_y_coord']\n",
    "\n",
    "# converting the given field's values to a numeric, or NaN, for possible rounding later\n",
    "try: \n",
    "    for field in spatial_coords:\n",
    "        #print(\"in the for loop\")\n",
    "        if (field in incoming_fields) or (field in target_fields):\n",
    "            #print(\"converting %s\" % field)\n",
    "            df[field] = pd.to_numeric(df[field], errors='coerce')\n",
    "    if ('input_x_coord' in incoming_fields) and ('input_y_coord' in incoming_fields):\n",
    "        coord_mode, decimal_place = coordinate_matcher(['x', 'y'], df, output_fields)\n",
    "    elif ('input_x_coord' in incoming_fields) and not ('input_y_coord' in incoming_fields):\n",
    "        coord_mode, decimal_place = coordinate_matcher(['x'], df, output_fields)\n",
    "    elif not ('input_x_coord' in incoming_fields) and ('input_y_coord' in incoming_fields):\n",
    "        coord_mode, decimal_place = coordinate_matcher(['y'], df, output_fields)\n",
    "    else:\n",
    "        print(\"\\nNo spatial coordinate fields available for matching.\")\n",
    "except KeyError:\n",
    "    print(\"Spatial coordinates not properly entered. Skipping coordinate matching.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The field 'input_type_py' could not be found; matching administrative type on field 'input_type_ch'\n"
     ]
    }
   ],
   "source": [
    "### handling administrative type matching\n",
    "if ('input_type_ch' in incoming_fields) and ('input_type_py' in incoming_fields):\n",
    "    print('''Administrative type information found in incoming data. Input the number for the field you want to match on:\n",
    "        1. Pinyin ('Xian', 'Zhou', etc)\n",
    "        2. Chinese ('县', '州', etc)\n",
    "    ''')\n",
    "    \n",
    "    type_choice = input()\n",
    "    accepted = False\n",
    "    \n",
    "    while accepted == False:\n",
    "        if (type_choice) == '1':\n",
    "            df['out_type_py_match'] = df['tgaz_type_py'] == df['input_type_py']\n",
    "            accepted = True\n",
    "            type_key = 'type_py'\n",
    "            output_fields += ['out_type_py_match']\n",
    "        elif type_choice == '2':\n",
    "            df['out_type_ch_match'] = df['tgaz_type_ch'] == df['input_type_ch']\n",
    "            accepted = True\n",
    "            type_key = 'type_ch'\n",
    "            output_fields += ['out_type_ch_match']\n",
    "        else:\n",
    "            print('Not a valid choice, please try again:')\n",
    "            type_choice = input()\n",
    "            \n",
    "elif ('input_type_ch' in incoming_fields) and ('input_type_py' not in incoming_fields):\n",
    "    print(\"The field 'input_type_py' could not be found; matching administrative type on field 'input_type_ch'\")\n",
    "    df['out_type_ch_match'] = df['tgaz_type_ch'] == df['input_type_ch']\n",
    "    type_key = 'type_ch'\n",
    "    output_fields += ['out_type_ch_match']\n",
    "    \n",
    "elif ('input_type_py' in incoming_fields) and ('input_type_ch' not in incoming_fields):\n",
    "    print(\"The field 'input_type_ch' could not be found; matching administrative type on field 'input_type_py'\")\n",
    "    df['out_type_py_match'] = df['tgaz_type_py'] == df['input_type_py']\n",
    "    type_key = 'type_py'\n",
    "    output_fields += ['out_type_py_match']\n",
    "    \n",
    "else:\n",
    "    print(\"Administrative type (e.g. 'Xian' or '县') could not be found in incoming data -- matching will not be attempted.\")\n",
    "    type_key = None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "############## NEW ################\n",
    "#if (df['input_year_beg'].empty() | df['input_year_end'].empty()):\n",
    "#    if (df['input_year_beg'].empty()):\n",
    "#        df.drop(['input_year_beg'])\n",
    "#        input_fields.remove('input_year_beg')\n",
    "#        beg_year_matching = False\n",
    "#    if (df['input_year_end'].empty()):\n",
    "#        df.drop(['input_year_end'])\n",
    "#        input_fields.remove('input_year_end')\n",
    "#        end_year_matching = False\n",
    "\n",
    "if (not 'input_year_beg' in incoming_fields) or (not 'input_year_end' in incoming_fields):\n",
    "    print(\"Incoming data lacks a beginning and/or ending year field; no date comparisons will be made.\")\n",
    "else:\n",
    "    # beg_year_matching, end_year_matching = True, True\n",
    "    \n",
    "    year_fields = ['input_year_beg', 'input_year_end', 'tgaz_beg', 'tgaz_end']\n",
    "    \n",
    "    #for field in year_fields:\n",
    "    #    print(df[field].dtype)\n",
    "           \n",
    "    # overlap field initialization\n",
    "    df['out_year_overlap'] = ''\n",
    "\n",
    "           \n",
    "    # converting to type float64 (invalid years become 'NaN', and pandas.isnull(<Series_(column)>) returns True for those values\n",
    "    for year_field in year_fields:\n",
    "        df[year_field] = pd.to_numeric(df[year_field], errors='coerce')\n",
    "    \n",
    "    # in-row match testing\n",
    "    df['out_beg_match'] = df['input_year_beg'] == df['tgaz_beg']\n",
    "    df['out_end_match'] = df['input_year_end'] == df['tgaz_end']\n",
    "    \n",
    "    # testing for timespan relationships\n",
    "    df['out_year_overlap'][((df['input_year_beg'] == (df['tgaz_end'] + 1)) | (df['input_year_end'] == (df['tgaz_beg'] - 1)))] = 'adjacent'\n",
    "    df['out_year_overlap'][(df['input_year_beg'] <= df['tgaz_beg']) & (df['input_year_end'] >= df['tgaz_beg']) & (df['input_year_end'] < df['tgaz_end'])] = 'partial_incl_start_of_target'\n",
    "    df['out_year_overlap'][(df['input_year_beg'] > df['tgaz_beg']) & (df['input_year_beg'] <= df['tgaz_end']) & (df['input_year_end'] >= df['tgaz_end'])] = 'partial_incl_end_of_target'\n",
    "    df['out_year_overlap'][((df['input_year_beg'] >= df['tgaz_beg']) & (df['input_year_end'] < df['tgaz_end'])) | ((df['input_year_beg'] > df['tgaz_beg']) & (df['input_year_end'] <= df['tgaz_end']))] = 'incoming_nested_in_target'\n",
    "    df['out_year_overlap'][((df['input_year_beg'] <= df['tgaz_beg']) & (df['input_year_end'] > df['tgaz_end']) | (df['input_year_beg'] < df['tgaz_beg']) & (df['input_year_end'] >= df['tgaz_end']))] = 'target_nested_in_incoming'\n",
    "    df['out_year_overlap'][(df['input_year_beg'] == df['tgaz_beg']) & (df['input_year_end'] == df['tgaz_end'])] = 'perfect_match'\n",
    "    df['out_year_overlap'][(df['input_year_beg'] == 0) | (df['input_year_end'] == 0) | (df['tgaz_beg'] == 0) | (df['tgaz_end'] == 0)] = 'CAUTION__ZEROES'\n",
    "    df['out_year_overlap'][(df['input_year_beg'] > df['input_year_end']) | (df['tgaz_beg'] > df['tgaz_end'])] = 'ERROR__END_BEFORE_BEG'\n",
    "    \n",
    "    # designating items with non-numeric text values in at least one year field (which therefore break the overlap checker)\n",
    "    df['out_year_overlap'][(df['match'] == 'found') & (pd.isnull(df['input_year_beg']) | pd.isnull(df['input_year_end']) | pd.isnull(df['tgaz_beg']) | pd.isnull(df['tgaz_end']))] = 'ERROR__NON_NUMERIC_YEAR_VALUE'\n",
    "    \n",
    "    # updating output fields\n",
    "    output_fields += ['out_beg_match'] + ['out_end_match'] + ['out_year_overlap']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# adding the 'match_strength' column, leveraging the fact that Python True and False evaluate to 1 and 0 respectively when passed to int()\n",
    "df['out_content_match_strength'] = 0\n",
    "output_fields += ['out_content_match_strength']\n",
    "\n",
    "if ('input_x_coord' in incoming_fields):\n",
    "    if coord_mode == \"strict\":\n",
    "        df['out_content_match_strength'] += df['out_x_coord_match'].astype(int)\n",
    "    else: \n",
    "        df['out_content_match_strength'] += df['fuzzy_out_x_coord_match'].astype(int)\n",
    "\n",
    "if ('input_y_coord' in incoming_fields):\n",
    "    if coord_mode == \"strict\":\n",
    "        df['out_content_match_strength'] += df['out_y_coord_match'].astype(int)\n",
    "    else: \n",
    "        df['out_content_match_strength'] += df['fuzzy_out_y_coord_match'].astype(int)\n",
    "    \n",
    "for field in ['out_beg_match', 'out_end_match', 'out_type_ch_match', 'out_type_py_match']:\n",
    "    if field in list(df.columns):\n",
    "        df['out_content_match_strength'] += df[field].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(target_fields)\n",
    "print(incoming_fields)\n",
    "print(output_fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sorting columns\n",
    "#print(output_fields)\n",
    "ordered_fields = target_fields + incoming_fields + ['match'] + output_fields\n",
    "#print(ordered_fields)\n",
    "fuzzy_ordered_fields = ['fuzzy_nm'] + ordered_fields\n",
    "\n",
    "if name_mode == 'strict':\n",
    "    df = df[ordered_fields]\n",
    "else:\n",
    "    df = df[fuzzy_ordered_fields]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(incoming_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# replacing 'nan' with '' for improved legibility\n",
    "df = df.replace('nan', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data check is complete.  Please type a path (without extension) for your output files:\n",
      "\n",
      "160925test14\n",
      "Thank you. Saving results.\n",
      "\n",
      "Match results stored in 160925test14.csv \n",
      "\n",
      "Matching info and summary of results stored in 160925test14.info.txt \n",
      "\n",
      "Now exiting.\n"
     ]
    }
   ],
   "source": [
    "# outputting results\n",
    "print('\\nData check is complete.  Please type a path (without extension) for your output files:\\n')\n",
    "output_path = input()\n",
    "accepted = False\n",
    "\n",
    "# validating output_path\n",
    "#while accepted == False:\n",
    "    # CHECK THAT FILENAMES DON'T ALREADY EXIST\n",
    "    # CHECK THAT PATH IS VALID\n",
    "\n",
    "print(\"Thank you. Saving results.\")  \n",
    "\n",
    "# writing the DataFrame to a .csv file at the specified output path while dropping the unlabeled index column that pandas DataFrames generate by default\n",
    "df.to_csv(\"%s.csv\" % output_path, index=False)\n",
    "\n",
    "# creating the summary .info.txt file\n",
    "summary_file = open(\"%s.info.txt\" % output_path, \"w\")\n",
    "\n",
    "# writing boilerplate \n",
    "summary_file.write(\"SUMMARY OF RESULTS\\nGenerated at %s\\n\\n\" % datetime.now().strftime(\"%H:%M, %m/%d/%Y\"))\n",
    "\n",
    "# writing compared & output filenames\n",
    "summary_file.write(\"Incoming file: %s\\n\" % incoming_name)\n",
    "summary_file.write(\"Target file: %s\\n\" % target_name)\n",
    "summary_file.write(\"Output file: %s.csv\\n\\n\" % os.path.basename(output_path))\n",
    "\n",
    "# writing basic statistics\n",
    "summary_file.write(\"Rows in incoming file: %s\\n\" % str(len(incoming.index)))\n",
    "summary_file.write(\"Rows in target file: %s\\n\" % str(len(target.index)))\n",
    "summary_file.write(\"Rows in output file: %s\\n\\n\\n\\n\" % str(len(df.index)))\n",
    "\n",
    "summary_file.write(\"FREQUENCY COUNTS\\n\")\n",
    "summary_file.write(\"Counts given for all values; 'Name' is the field name in the output file, and 'dtype' simply indicates the type of the counts (i.e. integers)\\n\\n\")\n",
    "summary_file.write(\"Matches by name: \\n\")\n",
    "summary_file.write(str(df['match'].value_counts()))\n",
    "summary_file.write(\"\\n\\n\")\n",
    "\n",
    "if ('input_x_coord' in incoming_fields):\n",
    "    if coord_mode == \"strict\":\n",
    "        summary_file.write(\"X coordinate matches: \\n\")\n",
    "        summary_file.write(str(df['out_x_coord_match'].value_counts()))\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "    else:\n",
    "        summary_file.write(\"Fuzzy x coordinate matches: \\n\")\n",
    "        summary_file.write(str(df['fuzzy_out_x_coord_match'].value_counts()))\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "if ('input_y_coord' in incoming_fields):\n",
    "    if coord_mode == \"strict\":\n",
    "        summary_file.write(\"Y coordinate matches: \\n\")\n",
    "        summary_file.write(str(df['out_y_coord_match'].value_counts()))\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "    else:\n",
    "        summary_file.write(\"Fuzzy y coordinate matches: \\n\")\n",
    "        summary_file.write(str(df['fuzzy_out_y_coord_match'].value_counts()))\n",
    "        summary_file.write(\"\\n\\n\")\n",
    "        \n",
    "if 'out_beg_match' in list(df.columns):\n",
    "    summary_file.write(\"Beginning year matches: \\n\")\n",
    "    summary_file.write(str(df['out_beg_match'].value_counts()))\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "if 'out_end_match' in list(df.columns):\n",
    "    summary_file.write(\"Ending year matches: \\n\")\n",
    "    summary_file.write(str(df['out_end_match'].value_counts()))\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "if 'out_year_overlap' in list(df.columns):\n",
    "    summary_file.write(\"Year overlaps: \\n\")\n",
    "    summary_file.write(str(df['out_year_overlap'].value_counts()))\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "    \n",
    "if 'out_type_ch_match' in list(df.columns):\n",
    "    summary_file.write(\"Administrative type match (Chinese): \\n\")\n",
    "    summary_file.write(str(df['out_type_ch_match'].value_counts()))\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "if 'out_type_py_match' in list(df.columns):\n",
    "    summary_file.write(\"Administrative type match (pinyin): \\n\")\n",
    "    summary_file.write(str(df['out_type_py_match'].value_counts()))\n",
    "    summary_file.write(\"\\n\\n\")\n",
    "\n",
    "summary_file.write(\"Content match strengths: \\n\")\n",
    "summary_file.write(str(df['out_content_match_strength'].value_counts()))\n",
    "summary_file.write(\"\\n\\n\\n\\n\")\n",
    "\n",
    "# writing information about match\n",
    "\n",
    "summary_file.write(\"BACKGROUND INFORMATION\\n\\n\")\n",
    "summary_file.write(\"The name match key was %s \\n\" % match_key)\n",
    "summary_file.write(\"The name match mode was %s \\n\" % name_mode)\n",
    "#summary_file.write(\"Report created at %s \\n\" % str(datetime.now)) \n",
    "type_key = None\n",
    "if type_key:\n",
    "    summary_file.write(\"The administrative type match key was %s\\n\" % type_key)\n",
    "summary_file.write(\"The coordinate match mode was %s \\n\" % coord_mode)\n",
    "if coord_mode == 'fuzzy':\n",
    "    summary_file.write(\"Coordinates were rounded to %s decimal place(s)\" % decimal_place)\n",
    "summary_file.write(\"\\n\\n\")\n",
    "summary_file.write(\"The target file's fields were renamed as follows (not all fields are included in final output):\\n\") \n",
    "with summary_file as out:\n",
    "    pprint(target_mapping, stream=out)\n",
    "    \n",
    "# reopening because pprint automatically closes file upon completion\n",
    "summary_file = open(\"%s.info.txt\" % output_path, \"a\")\n",
    "summary_file.write(\"\\n\\n\")\n",
    "summary_file.write(\"The incoming file's fields were renamed as follows (not all fields are included in final output):\\n\")\n",
    "\n",
    "with summary_file as out:\n",
    "    pprint(incoming_mapping, stream=out)\n",
    "\n",
    "summary_file = open(\"%s.info.txt\" % output_path, \"a\")\n",
    "summary_file.write(\"\\n\\n\")\n",
    "summary_file.write(\"The actual fields used in the output file are:\\n\")\n",
    "with summary_file as out:\n",
    "    pprint(list(df.columns), stream=out)\n",
    "\n",
    "summary_file = open(\"%s.info.txt\" % output_path, \"a\")\n",
    "summary_file.write(\"\\n\\n\\n\")\n",
    "summary_file.write(\"Report generated using geoname_match.py \\nQuestions or concerns? Contact Stephen Ford (stephen.p.ford@gmail.com)\")\n",
    "\n",
    "# closing file\n",
    "summary_file.close()\n",
    "\n",
    "print(\"\\nMatch results stored in %s.csv \\n\\nMatching info and summary of results stored in %s.info.txt \\n\\nNow exiting.\" % (os.path.basename(output_path), os.path.basename(output_path)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
