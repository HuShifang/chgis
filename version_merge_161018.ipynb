{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#! /usr/bin/python3\n",
    "# \n",
    "# /version_merge_160822.ipynb\n",
    "# /py_scripts/version_merge_160822.py\n",
    "#\n",
    "# A script for checking data integrity of CHGIS v5 and v6.  \n",
    "# The script takes a v5 .csv provided by Lex Berman and a v6 .csv generated using v6_consolidation_160814.ipynb (or the same-name .py script)\n",
    "# The .csv files are internally unique -- that is, there are no duplicate items within them -- and have several (but not all) fields in common\n",
    "# This script checks for duplicates between them, based on various fields including 'sys_id'\n",
    "# Columns containing Boolean values are appended to the end of rows, indicating whether fields are duplicates or not\n",
    "# There are several \"meta\" columns at the end, marking out rows that may be of interest\n",
    "# Note that while there are several rows where all relevant data save for 'sys_id' match, there do not appear to be any where only the 'sys_id' matches\n",
    "# The script has not been fully refactored, and can yet be optimized; however, it should still run relatively quickly.\n",
    "# Three .csv files are output -- one containing the entirety of both v5 and v6 together, one containing just v5 rows, and one containing just v6 rows\n",
    "\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "import numpy as np\n",
    "import os, os.path\n",
    "\n",
    "# suppressing SettingWithCopyWarning\n",
    "pd.options.mode.chained_assignment = None  # default='warn'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# function for selecting .csv files for manipulation\n",
    "def csv_picker():\n",
    "    ''' Function for checking whether user input path 1) is that of a valid file, and 2) is of a file ending with '.csv'\n",
    "        Prompts for re-entry if entry is invalid.\n",
    "        Returns a pandas DataFrame constructed from the valid .csv file\n",
    "    '''\n",
    "    path_name = input()\n",
    "\n",
    "    # checking that the path is a valid filename, and prompting for re-entry if not\n",
    "    while not (os.path.isfile(path_name)):\n",
    "        print(\"Not a valid filename.  Please try again:\")\n",
    "        path_name = input()\n",
    "        \n",
    "    # checking that the valid filename ends in .csv, prompting for re-entry if not\n",
    "    while not path_name.endswith('.csv'):\n",
    "        print(\"Filename does not end in .csv -- please try again:\")\n",
    "        path_name = input()\n",
    "\n",
    "    print(\"\\nThank you -- path %s is valid.\\n\" % path_name)\n",
    "    \n",
    "    # storing only the file's basename, for use in user prompts\n",
    "    name = os.path.basename(path_name)\n",
    "    return pd.read_csv(path_name, low_memory=False), name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please type the path of the v5 .csv file (with extension):\n",
      "\n",
      "\n",
      "Not a valid filename.  Please try again:\n",
      "/home/sf/chgis/input/v5_augment_2016-08-09.csv\n",
      "\n",
      "Thank you -- path /home/sf/chgis/input/v5_augment_2016-08-09.csv is valid.\n",
      "\n",
      "Please type the path of the v6 .csv file (with extension):\n",
      "\n",
      "/home/sf/chgis/input/v6_GISInfoTable_2016-08-09.csv\n",
      "\n",
      "Thank you -- path /home/sf/chgis/input/v6_GISInfoTable_2016-08-09.csv is valid.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# importing v5 and v6 tables as DataFrames\n",
    "print(\"Please type the path of the v5 .csv file (with extension):\\n\")\n",
    "v5, v5_name = csv_picker()\n",
    "\n",
    "print(\"Please type the path of the v6 .csv file (with extension):\\n\")\n",
    "v6, v6_name = csv_picker()\n",
    "\n",
    "# creating a field indicating which version of CHGIS the given row came from\n",
    "v5['version'] = 5\n",
    "v6['version'] = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proceeding with merge and check of v5_augment_2016-08-09.csv and v6_GISInfoTable_2016-08-09.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"Proceeding with merge and check of %s and %s\" % (v5_name, v6_name))\n",
    "# merge v5 and v6 into a single DataFrame\n",
    "df = pd.concat([v5,v6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# check merged df for duplicates by sys_id, generate file with tagging\n",
    "mask_duplicates = df.duplicated(subset='sys_id', keep=False)\n",
    "mask_uniques = ~mask_duplicates\n",
    "duplicates = df[mask_duplicates]\n",
    "duplicates['sys_id_duplicate'] = True\n",
    "uniques = df[mask_uniques]\n",
    "uniques['sys_id_duplicate'] = False\n",
    "df_two = pd.concat([duplicates, uniques])\n",
    "df_two.to_csv('output/v5_and_v5_duplicates_and_uniques.csv')\n",
    "\n",
    "### following lines would output .csv's containing only the duplicates and uniques\n",
    "#duplicates.to_csv('output/v5_and_v6_primary_duplicates.csv')\n",
    "#uniques.to_csv('v5_and_v6_uniques.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function for adding Boolean flags as fields to the DataFrame file\n",
    "def field_matcher(frame, v6_field, v5_field, new_field):\n",
    "    '''Simple function for comparing the specified fields from versions 5 and 6 of the CHGIS within the specified composite frame, outputting the Boolean result to a new field.  \n",
    "    Assumes that the CHGIS versions' tables which have been concatenated into the frame are already internally unique, i.e. that any matches found will be between versions, not within.  \n",
    "    Rather than iterate over rows, creates a Series of Boolean values which serves as a sort of mask for filtering out rows with duplicate and unique values in the specified fields. -- SF, 160817'''\n",
    "    mask_duplicates = frame.duplicated(subset=[v6_field, v5_field], keep=False)\n",
    "    mask_uniques = ~mask_duplicates\n",
    "    duplicates = frame[mask_duplicates]\n",
    "    duplicates[new_field] = True\n",
    "    uniques = frame[mask_uniques]\n",
    "    uniques[new_field] = False\n",
    "    return pd.concat([duplicates, uniques])        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# performing duplicate checks field-by-field per 'Match_fields_from_V6_to_V5.csv'\n",
    "df_three = field_matcher(df_two, 'nm_py', 'nm_py', 'nm_py_duplicate')\n",
    "df_four = field_matcher(df_three, 'nm_simp', 'nm_simp', 'nm_simp_duplicate')\n",
    "df_five = field_matcher(df_four, 'nm_trad', 'nm_trad', 'nm_trad_duplicate')\n",
    "\n",
    "# trying to workaround failure to recognize matches\n",
    "df_five['x_coord'] = df_five['x_coord'].astype(str)\n",
    "df_five['y_coord'] = df_five['y_coord'].astype(str)\n",
    "df_six = field_matcher(df_five, 'x_coord', 'x_coord', 'x_coord_duplicate')\n",
    "df_seven = field_matcher(df_six, 'y_coord', 'y_coord', 'y_coord_duplicate')\n",
    "df_seven['x_coord'] = df_seven['x_coord'].astype(float)\n",
    "df_seven['y_coord'] = df_seven['y_coord'].astype(float)\n",
    "\n",
    "# resuming secondary checks\n",
    "df_eight = field_matcher(df_seven, 'pres_loc', 'pres_loc', 'pres_loc_duplicate')\n",
    "df_nine = field_matcher(df_eight, 'type_py', 'type_py', 'type_py_duplicate')\n",
    "df_ten = field_matcher(df_nine, 'type_simp', 'type_ch', 'type_simp/ch_duplicate')\n",
    "df_eleven = field_matcher(df_ten, 'beg_yr', 'beg', 'beg_yr_duplicate')\n",
    "df_twelve = field_matcher(df_eleven, 'end_yr', 'end', 'end_yr_duplicate')\n",
    "df_thirteen = field_matcher(df_twelve, 'obj_type', 'obj_type', 'obj_type_duplicate')\n",
    "df_thirteen['exact_match'] = df_thirteen.all(axis=1, bool_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### creating new columns for ease of filtering\n",
    "\n",
    "# NOTE that this can probably be refactored to use a slice of df_thirteen columns; \n",
    "# I was having problems slicing it right, however, so I used this less elegant approach, explicitly checking each field's value\n",
    "# The script runs sufficiently quickly as-is, however\n",
    "\n",
    "# Note that numpy.where() is used here, and NOT pandas.where() \n",
    "\n",
    "## if 'sys_id_duplicate' == False, but the other duplicate fields == True, then 'content_only' = True\n",
    "df_thirteen['content_only'] = np.where(\n",
    "    ((df_thirteen['sys_id_duplicate'] == False) &\n",
    "    ((df_thirteen['nm_py_duplicate'] == True) & \n",
    "    (df_thirteen['nm_simp_duplicate'] == True) & \n",
    "    (df_thirteen['nm_trad_duplicate'] == True) &\n",
    "    (df_thirteen['x_coord_duplicate'] == True) &\n",
    "    (df_thirteen['y_coord_duplicate'] == True) &\n",
    "    (df_thirteen['pres_loc_duplicate'] == True) &\n",
    "    (df_thirteen['type_py_duplicate'] == True) &\n",
    "    (df_thirteen['type_simp/ch_duplicate'] == True) &\n",
    "    (df_thirteen['beg_yr_duplicate'] == True) &\n",
    "    (df_thirteen['end_yr_duplicate'] == True) &\n",
    "    (df_thirteen['obj_type_duplicate'] == True))), 'True', 'False') \n",
    "\n",
    "## if 'sys_id'duplicate' == True, but the other duplicate fields == False, then 'id_only' = True\n",
    "df_thirteen['id_only'] = np.where(\n",
    "    ((df_thirteen['sys_id_duplicate'] == True) & \n",
    "    ((df_thirteen['nm_py_duplicate'] == False) & \n",
    "    (df_thirteen['nm_simp_duplicate'] == False) & \n",
    "    (df_thirteen['nm_trad_duplicate'] == False) &\n",
    "    (df_thirteen['x_coord_duplicate'] == False) &\n",
    "    (df_thirteen['y_coord_duplicate'] == False) &\n",
    "    (df_thirteen['pres_loc_duplicate'] == False) &\n",
    "    (df_thirteen['type_py_duplicate'] == False) &\n",
    "    (df_thirteen['type_simp/ch_duplicate'] == False) &\n",
    "    (df_thirteen['beg_yr_duplicate'] == False) &\n",
    "    (df_thirteen['end_yr_duplicate'] == False) &\n",
    "    (df_thirteen['obj_type_duplicate'] == False))), 'True', 'False') \n",
    "\n",
    "df_final = df_thirteen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please provide a path to the folder where the output will go:\n",
      "\n",
      "/home/sf/chgis/output\n"
     ]
    }
   ],
   "source": [
    "print(\"Please provide a path to the folder where the output will go:\\n\")\n",
    "output_location = input()\n",
    "\n",
    "while not (os.path.isdir(output_location)):\n",
    "    print(\"Invalid response.  Please try again:\\n\")\n",
    "    output_location = input()\n",
    "\n",
    "os.chdir(output_location)\n",
    "\n",
    "# outputting .csv with v5 AND v6\n",
    "df_final.to_csv('V5_and_V6_output.csv')\n",
    "\n",
    "# masking, outputting v5 alone\n",
    "v5_mask = df_final['version'] == 5\n",
    "v5_final = df_final[v5_mask]\n",
    "v5_final.to_csv('V5_output.csv')\n",
    "\n",
    "# masking, outputting v6 alone\n",
    "v6_mask = df_final['version'] == 6\n",
    "v6_final = df_final[v6_mask]\n",
    "v6_final.to_csv('V6_output.csv')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
